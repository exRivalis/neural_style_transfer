{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load a pretrained image classification model\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "import keras.backend as kb\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First extract and store content and style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape\n",
    "im_shape = (224, 224, 3)\n",
    "# content\n",
    "content_path = 'best-artworks-of-all-time/images/images/Vincent_van_Gogh/Vincent_van_Gogh_27.jpg'\n",
    "content_image = image.load_img(content_path, target_size=(im_shape[0], im_shape[1]))\n",
    "# original image\n",
    "content = image.img_to_array(content_image)\n",
    "content = np.expand_dims(content, axis=0)\n",
    "# create keras constant for content image\n",
    "content = kb.constant(preprocess_input(content))\n",
    "\n",
    "# style\n",
    "style_path = 'data/normal_Le_lac_de_Montriond_0.jpg'\n",
    "style_image = image.load_img(style_path, target_size=im_shape)\n",
    "style = image.img_to_array(style_image)\n",
    "style = np.expand_dims(style, axis=0)\n",
    "# create a keras constant for style image\n",
    "style = kb.constant(preprocess_input(style))\n",
    "\n",
    "# generated image\n",
    "# x = np.random.randint(0, 256, im_shape)\n",
    "# x = np.ones(im_shape)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# create a keras placehoder generated image\n",
    "target = kb.placeholder((1, im_shape[0], im_shape[1], im_shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make an input tensor from it all\n",
    "input_tensor = kb.concatenate([target, content, style], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG19(weights='imagenet', input_tensor=input_tensor, include_top=False)\n",
    "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_conv2').output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style transfer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify which layers we want to use for style transfer. <br>\n",
    "From the style image we will compute a style representation using the netwrok. Let's call this representation <b><i>Al</i></b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a <i>style</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The style of an image is defined as the correlation between its different channels of the same layer.<br>\n",
    "We want our target image to have the same correlation as the style image, that is the same <i>style</i>.<br>\n",
    "To do so we need to update our image pixels in such way that the difference between style representations of our target image on one hand and the style image on the other hand be small.<br>\n",
    "To measure this difference we're going to define a cost function.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this, we need to compute the correlation between every two channels of the same layer. That is exactly what the Gram matrix does.<br>\n",
    "The Gram matrix is the matrix of all possible inner products, \n",
    "i.e. \\begin{equation*} g_{ij} = {v_i}^Tv_j \\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    x = kb.permute_dimensions(x, (2, 0, 1))\n",
    "    return kb.dot(x, kb.transpose(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the style loss function, one part of our final cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{1}{4N_l^2M_l^2}\\sum_{i,j}(G_{ij}^l-A_{ij}^l)^2\n",
    "\\end{equation*}\n",
    "with:<br>\n",
    "<b>N</b>: number of filters or feature maps<br>\n",
    "<b>M</b>: size of a feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, target):\n",
    "    s = gram_matrix(style)\n",
    "    t = gram_matrix(target)\n",
    "    tot = kb.sum(kb.square(s-t))\n",
    "    N2 = im_shape[2]**2\n",
    "    M2 = (im_shape[0]*im_shape[1])**2\n",
    "    \n",
    "    return tot / (4. * N2 * M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content loss, the other part of our final cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the target image to look like the content image. For this we measure similarity between their feature representation matrices.<br>\n",
    "We don't want the target image to be a copy of the content but to share only key features. Hence the measure should take place at an intermediate layer: too close to the input layer only weak features are learnt, too close to the output layer only strong features are learnt.<br>\n",
    "The studied paper advices the <i>block4_conv2</i> layer.That's exactly what we'll do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, target):\n",
    "    return kb.sum(kb.square(target - content)) / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is a combination of both cost functions. Minimizing it insures a target image that shares key features of the content image and style features of the style image.<br>\n",
    "One can prefer style over content or the other way around. This is controlled through <i>a</i> and <i>b</i>, style and content weights.<br>\n",
    "A high ratio <i>a/b</i> emphasizes on style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(target, style, content, a, b):\n",
    "    return a*content_loss(content, target) + b*style_loss(style, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's specify layers of interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['block1_conv2', 'block2_conv2', 'block3_conv2', 'block4_conv2', 'block5_conv2']\n",
    "content_layer = 'block4_conv2'\n",
    "# A = [Model(inputs=base_model.input, outputs=base_model.get_layer(layer).output).predict(a) for layer in style_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The style image <b><i>a</i></b> is passed through the network and its style representation <b><i>A</i></b> of all layers included are computed and stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember how we defined an input tensor and fed it to our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > input_tensor = kb.concatenate([target, content, style], axis=0) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract some features as follow (as named in the paper):<br>\n",
    "- <b>P</b>: content features from the content image\n",
    "- <b>A</b>: style features from the style image\n",
    "- <b>G</b>: Target image's style features\n",
    "- <b>F</b>: Target image's content features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del w_content, w_style, loss_content, loss_style, loss_tot\n",
    "# del A, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember input_tensor at position 1 is the content image\n",
    "P = model.get_layer(content_layer).output[1, :, :, :] # content\n",
    "F = model.get_layer(content_layer).output[0, :, :, :] # target\n",
    "\n",
    "# content and style weights\n",
    "w_content = 0.0001\n",
    "w_style = 0.9\n",
    "\n",
    "# we have enough info to compute our losses\n",
    "loss_content, loss_style, loss_tot = kb.variable(0.), kb.variable(0.), kb.variable(0.)\n",
    "\n",
    "loss_content = w_content * content_loss(P, F)\n",
    "# style image at position 2\n",
    "for layer in style_layers:\n",
    "    A = model.get_layer(layer).output[2, :, :, :] # style\n",
    "    G = model.get_layer(layer).output[0, :, :, :] # target\n",
    "    loss_style = loss_style + (w_style / len(style_layers)) * style_loss(A, G)\n",
    "\n",
    "loss_tot = loss_content + loss_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = kb.gradients(loss_tot, target)[0]\n",
    "loss_and_gradients = kb.function([target], [loss_tot, gradients])\n",
    "# apply loss_tot and gradients tensors to target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "    \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, im_shape[0], im_shape[1], im_shape[2]))\n",
    "        outs = loss_and_gradients([x])\n",
    "    \n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 6017288000.0\n",
      "Image saved as my_result_at_iteration_0.png\n",
      "Iteration 0 completed in 216s\n",
      "Start of iteration 1\n",
      "Current loss value: 3144883700.0\n",
      "Image saved as my_result_at_iteration_1.png\n",
      "Iteration 1 completed in 212s\n",
      "Start of iteration 2\n",
      "Current loss value: 2243999000.0\n",
      "Image saved as my_result_at_iteration_2.png\n",
      "Iteration 2 completed in 212s\n",
      "Start of iteration 3\n",
      "Current loss value: 1769144400.0\n",
      "Image saved as my_result_at_iteration_3.png\n",
      "Iteration 3 completed in 212s\n",
      "Start of iteration 4\n",
      "Current loss value: 1462278400.0\n",
      "Image saved as my_result_at_iteration_4.png\n",
      "Iteration 4 completed in 206s\n",
      "Start of iteration 5\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "# from scipy.misc import imsave\n",
    "import time\n",
    "\n",
    "result_prefix = 'my_result'\n",
    "iterations = 20\n",
    "\n",
    "content_path = 'best-artworks-of-all-time/images/images/Vincent_van_Gogh/Vincent_van_Gogh_27.jpg'\n",
    "content_image = image.load_img(content_path, target_size=(im_shape[0], im_shape[1]))\n",
    "# original image\n",
    "x = image.img_to_array(content_image)\n",
    "x = np.expand_dims(content, axis=0)\n",
    "\n",
    "# x = preprocess_image(target_image_path)\n",
    "x = x.flatten()\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
    "                                     x,\n",
    "                                     fprime=evaluator.grads,\n",
    "                                     maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    img = x.copy().reshape(im_shape)  \n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    imwrite(fname, img)\n",
    "    print('Image saved as', fname)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
